{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train3.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3d12b2bd9e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train3.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train3.txt'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train3.txt', unpack=True, dtype='float32')\n",
    "\n",
    "x_data = np.transpose(xy[0:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print (step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "0 0.705802 [[-0.8325181   0.75114942]\n",
      " [ 0.40126315  0.57999659]] [[ 0.23922026]\n",
      " [ 0.53223711]]\n",
      "200 0.531792 [[-1.27680087  0.72002387]\n",
      " [ 1.25830734  0.3591097 ]] [[ 1.54813457]\n",
      " [ 0.17480597]]\n",
      "400 0.436876 [[-1.71117902  0.99242365]\n",
      " [ 1.69894755 -0.21029091]] [[ 2.22755718]\n",
      " [ 0.64105201]]\n",
      "600 0.132907 [[-1.94334173  1.64411581]\n",
      " [ 1.93805099 -1.65255189]] [[ 2.58150744]\n",
      " [ 2.19137907]]\n",
      "800 0.0604655 [[-2.13223147  2.04015732]\n",
      " [ 2.13160491 -2.04180479]] [[ 2.86588264]\n",
      " [ 2.77261066]]\n",
      "1000 0.0374997 [[-2.27307892  2.22817969]\n",
      " [ 2.27272964 -2.22688413]] [[ 3.07433414]\n",
      " [ 3.04574347]]\n",
      "1200 0.0267241 [[-2.37869263  2.34492421]\n",
      " [ 2.37292337 -2.34741664]] [[ 3.22589827]\n",
      " [ 3.21872926]]\n",
      "1400 0.0204771 [[-2.45543766  2.43098569]\n",
      " [ 2.4537065  -2.43323827]] [[ 3.34159803]\n",
      " [ 3.34371161]]\n",
      "1600 0.016525 [[-2.51867247  2.49858522]\n",
      " [ 2.51638651 -2.49907374]] [[ 3.43389606]\n",
      " [ 3.44059944]]\n",
      "1800 0.0137858 [[-2.57027173  2.55324912]\n",
      " [ 2.5688386  -2.55300474]] [[ 3.51005936]\n",
      " [ 3.51935339]]\n",
      "[array([[ 0.0188034 ],\n",
      "       [ 0.99537402],\n",
      "       [ 0.9952814 ],\n",
      "       [ 0.0188034 ]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool)]\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xy = np.loadtxt('data/train_xor.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "print (x_data)\n",
    "print (y_data)\n",
    "\n",
    "X = tf.placeholder(tf.float32, name='x-input')\n",
    "Y = tf.placeholder(tf.float32, name='y-input')\n",
    "\n",
    "w1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0), name='weight1')\n",
    "w2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name='weight8')\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "L2 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, w2) + b2)\n",
    "\n",
    "with tf.name_scope('cost') as scope:\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope('train') as scope:\n",
    "    a = tf.Variable(0.1)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "w1_hist = tf.histogram_summary(\"weights1\", w1)\n",
    "w2_hist = tf.histogram_summary(\"weights2\", w2)\n",
    "\n",
    "b1_hist = tf.histogram_summary(\"biases1\", b1)\n",
    "b2_hist = tf.histogram_summary(\"biases2\", b2)\n",
    "\n",
    "y_hist = tf.histogram_summary(\"y\", Y)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"./logs/xor_logs\", sess.graph)\n",
    "\n",
    "    for step in range(2000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            summary = sess.run(merged, feed_dict={X: x_data, Y: y_data})\n",
    "            writer.add_summary(summary, step)\n",
    "            print (step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(w1), sess.run(w2))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction], feed_dict={X: x_data, Y: y_data}))\n",
    "    print (\"accuracy\", accuracy.eval({X: x_data, Y: y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "0 0.755881 [[ 0.42748186  0.25549844]\n",
      " [ 0.86219686  0.68550992]] [[ 0.98478162]\n",
      " [-0.09681578]]\n",
      "200 0.693516 [[ 0.25995129  0.29966202]\n",
      " [ 0.58419758  0.76989412]] [[ 0.67493623]\n",
      " [-0.39727598]]\n",
      "400 0.590821 [[ 0.27034035  0.78594625]\n",
      " [ 0.57572269  0.78884715]] [[ 0.67276913]\n",
      " [-1.14423978]]\n",
      "600 0.346938 [[ 0.87658292  1.54825878]\n",
      " [ 0.98837829  1.54729152]] [[ 1.33676863]\n",
      " [-2.55721521]]\n",
      "800 0.164984 [[ 1.43758488  2.12040329]\n",
      " [ 1.45260239  2.11304522]] [[ 2.0508728 ]\n",
      " [-3.57666063]]\n",
      "1000 0.0897341 [[ 1.74399495  2.46703339]\n",
      " [ 1.74669302  2.46885252]] [[ 2.47243738]\n",
      " [-4.19107533]]\n",
      "1200 0.0570183 [[ 1.93135393  2.68882251]\n",
      " [ 1.93269324  2.69036984]] [[ 2.73518443]\n",
      " [-4.58244705]]\n",
      "1400 0.0401754 [[ 2.06069136  2.84424448]\n",
      " [ 2.06149125  2.84502649]] [[ 2.91707778]\n",
      " [-4.85631084]]\n",
      "1600 0.0305037 [[ 2.15701509  2.96223378]\n",
      " [ 2.15757465  2.96062088]] [[ 3.05276132]\n",
      " [-5.06221199]]\n",
      "1800 0.0243565 [[ 2.2326827   3.05438709]\n",
      " [ 2.2330997   3.05486488]] [[ 3.15943193]\n",
      " [-5.2250576 ]]\n",
      "[array([[ 0.04880767],\n",
      "       [ 0.98875511],\n",
      "       [ 0.98874283],\n",
      "       [ 0.00788629]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool)]\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xy = np.loadtxt('data/train_xor.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "print (x_data)\n",
    "print (y_data)\n",
    "\n",
    "X = tf.placeholder(tf.float32, name='x-input')\n",
    "Y = tf.placeholder(tf.float32, name='y-input')\n",
    "\n",
    "w1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0), name='weight1')\n",
    "w2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0), name='weight8')\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "L2 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, w2) + b2)\n",
    "\n",
    "with tf.name_scope('cost') as scope:\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "    cost_summ = tf.scalar_summary(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope('train') as scope:\n",
    "    a = tf.Variable(0.1)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "w1_hist = tf.histogram_summary(\"weights1\", w1)\n",
    "w2_hist = tf.histogram_summary(\"weights2\", w2)\n",
    "\n",
    "b1_hist = tf.histogram_summary(\"biases1\", b1)\n",
    "b2_hist = tf.histogram_summary(\"biases2\", b2)\n",
    "\n",
    "y_hist = tf.histogram_summary(\"y\", Y)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    merged = tf.merge_all_summaries()\n",
    "    writer = tf.train.SummaryWriter(\"./logs/xor_logs\", sess.graph)\n",
    "\n",
    "    for step in range(2000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            summary = sess.run(merged, feed_dict={X: x_data, Y: y_data})\n",
    "            writer.add_summary(summary, step)\n",
    "            print (step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(w1), sess.run(w2))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction], feed_dict={X: x_data, Y: y_data}))\n",
    "    print (\"accuracy\", accuracy.eval({X: x_data, Y: y_data}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
